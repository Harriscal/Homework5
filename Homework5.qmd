---
title: "Homework 5"
author: "Calista Harris"
format: pdf
editor: visual
editor_options: 
  chunk_output_type: console
---

## Task 1: Conceptual Questions

### Question 1

Cross-validation helps us estimate how well a random forest model is likely to perform on unseen data by repeatedly splitting the data into training and validation sets. This process reduces variability in the performance estimate that could arise from a single train/test split and allows for more reliable model assessment. Although random forests have built-in error estimation through out-of-bag observations, cross-validation can provide a complementary or alternative method, especially when comparing multiple models or tuning hyperparameters.

### Question 2

The bagged tree algorithm, short for bootstrap aggregation, involves generating multiple bootstrap samples from the original dataset, fitting a decision tree to each sample, and aggregating their predictions. For regression tasks, the final prediction is the average of individual tree predictions, while for classification, it is typically determined by majority vote. This ensemble method helps reduce variance and improves predictive performance compared to a single tree model.

### Question 3

A general linear model (GLM) is a statistical model where the expected value of the response variable is modeled as a linear combination of the explanatory variables. It has the form $E(Y|X)=\beta_0 + \beta_1X_1 +\beta_2X_2 + \cdots +\beta_pX_p$, and is used for tasks such as multiple linear regression and ANOVA. The model assumes the errors are normally distributed and have constant variance, making it a foundational approach for modeling relationships between variables.

### Question 4

Adding an interaction term to a multiple linear regression model allows the effect of one explanatory variable on the response variable to depend on the level of another explanatory variable. This enables the model to capture relationships where the combined effect of two variables is not merely additive. In mathematical terms, it adds a term like $\beta_3X_1X_2$ to the model, allowing the slope of one variable to change based on the value of the other.

### Question 5

Splitting the data into a training and test set allows us to evaluate how well a model generalizes to new, unseen data. The training set is used to fit the model, while the test set provides an unbiased estimate of the model’s predictive performance. This helps guard against overfitting and ensures that the model is not simply memorizing the training data, but instead learning patterns that can apply more broadly.

\newpage

## Task 2: Data Prep

### Packages and Data

```{r Packages-Data}
#| message: false
#| warning: false
#load require libraries 
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)

#read in the heart disease dataset as a tibble
heart <- read_csv("data/heart.csv") |> 
  as_tibble()
```

### Question 1

```{r Task2-Question1}
#summarize the data
summary(heart)
```

a.  According to the summary() output, `HeartDisease` is currently treated as a numeric variable in R. This is evident from the statistical summaries displayed — Min, 1st Qu., Median, Mean, etc. — all indicators of a quantitative numeric type.

b.  No, this does not make sense for modeling. The `HeartDisease` variable encodes binary outcomes — either 0 (no heart disease) or 1 (presence of heart disease). As described in Logistic Regression Models, binary outcomes should be treated as categorical when modeling classification problems. Using it as numeric may lead to inappropriate modeling choices, such as applying linear regression when logistic regression is more appropriate.

### Question 2
```{r Task2-Question2}
#convert HeartDisease to a factor (categorical) variable and rename it
new_heart <- heart |> 
  mutate(HeartDisease_status = factor(HeartDisease)) |> 
  #drop the original numeric HeartDisease variable and the ST_Slope variable
  select(-ST_Slope, -HeartDisease) 

#view the structure of the updated data set
glimpse(new_heart)
```

\newpage

## Task 3: EDA

### Question 1
```{r Task3-Question1}
#create the scatterplot with separate trend lines by heart disease
ggplot(new_heart, aes(x = MaxHR, y = Age, color = HeartDisease_status)) +
  geom_point(alpha = 0.5) +
  #remove the standard error bars 
  geom_smooth(method = "lm", se = FALSE) + 
  #Set1 is color-blind friendly 
  scale_color_brewer(palette = "Set1", name = "Heart Disease") + 
  labs(
    title = "Interaction Between Max Heart Rate and Heart Disease in Age",
    x = "Max Heart Rate",
    y = "Age"
  ) +
  theme_minimal()
```

### Question 2
The scatterplot shows non-parallel trend lines for people with and without heart disease, indicating that the relationship between Max Heart Rate and Age differs by Heart Disease. Specifically, the slope for individuals without heart disease is steeper than for those with heart disease. This suggests an interaction effect, where the impact of `MaxHR` on `Age` depends on `HeartDisease_status `. Recall Multiple Linear Regression, interaction terms allow the slope of one variable to change based on the level of another. Therefore, an interaction model is more appropriate than an additive model for this analysis.

\newpage 

## Task 4: Testing and Training 
```{r Task4}
#set seed for reproducibility
set.seed(101)

#perform 80-20 train-test split
heart_split <- initial_split(new_heart, prop = 0.8)

#create training and test data sets
train <- training(heart_split)
test <- testing(heart_split)

#check sizes of each set
nrow(train)  
nrow(test)  
```

\newpage

## Task 5: OLS and LASSO

### Question 1
```{r Task5-Question1}
#fit OLS model with interaction between MaxHR and HeartDisease
ols_mlr <- lm(Age ~ MaxHR * HeartDisease_status, data = train)

#report the summary output of the model
summary(ols_mlr)
```

### Question 2
```{r Task5-Question2}
#predict response (Age) on test data using the OLS model
pred_ols <- predict(ols_mlr, newdata = test)

#bind predictions to test data for RMSE evaluation
ols_results <- test |> 
  mutate(pred = pred_ols)

#compute RMSE using yardstick
ols_rmse <- rmse(ols_results, truth = Age, estimate = pred) 
ols_rmse
```

### Question 3
```{r Task5-Question3}
#(a-d) Define the recipe with normalization, dummy coding, and interaction
LASSO_recipe <- recipe(Age ~ MaxHR + HeartDisease_status, data = train) |>
  step_normalize(all_numeric_predictors()) |>   #(b) standardize
  step_dummy(all_nominal_predictors()) |>       #dummy encode
  step_interact(terms = ~ MaxHR:starts_with("HeartDisease_status")) #(c & d)

#print recipe
LASSO_recipe
```

### Question 4
```{r Task5-Question4}
#specify LASSO model
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

#combine recipe and model
LASSO_wflow <- workflow() |>
  add_recipe(LASSO_recipe) |>
  add_model(LASSO_spec)

#cross-validation folds
set.seed(101)
LASSO_folds <- vfold_cv(train, v = 10)

#tune penalty on 10-fold CV
LASSO_tuned <- tune_grid(
  object = LASSO_wflow,
  resamples = LASSO_folds,
  grid = 25,                    #try 25 penalty values
  metrics = metric_set(rmse)
)

#select best-performing penalty
best_penalty <- select_best(LASSO_tuned, metric = "rmse")

#finalize workflow
final_LASSO <- finalize_workflow(LASSO_wflow, best_penalty)

#fit to training data
LASSO_fit <- fit(final_LASSO, data = train)

#report the results
tidy(LASSO_fit)
```

### Question 5
Based on the output from the LASSO model in Question 4, we would expect the RMSE to be roughly similar to that of the OLS model. This is because all variables—including the interaction between `MaxHR` and `HeartDisease_status` have non-zero coefficients in the LASSO model. Since LASSO didn’t shrink any coefficients to zero (which it often does for variable selection), it suggests that the model complexity and fit are comparable to OLS. Therefore, the predictive performance, as measured by RMSE, is likely to be similar.

### Question 6
```{r Task5-Question6}
#generate predictions on the test set using the final LASSO model
LASSO_preds <- predict(LASSO_fit, new_data = test) |> 
  bind_cols(test)  #attach predictions to the actual test data

#calculate RMSE for the LASSO model using yardstick
LASSO_rmse <- rmse(LASSO_preds, truth = Age, estimate = .pred)

#create a tibble comparing RMSE between OLS and LASSO models
compare_rmse <- tibble(
  Model = c("OLS", "LASSO"),
  RMSE = c(ols_rmse$.estimate, LASSO_rmse$.estimate)
)

#display the RMSE comparison
compare_rmse
```

### Question 7
Even though the OLS and LASSO models have different coefficient estimates, their RMSE values are roughly the same. This is because both models capture the same underlying relationship between `Age`, `MaxHR`, and `HeartDisease_status`. LASSO applies a penalty that shrinks coefficients, while OLS does not. In our results, the key predictors and their interaction were retained in both models. The LASSO coefficients are smaller due to regularization, but the predictive patterns are similar. LASSO aims to improve model generalization without drastically changing predictions when the signal is strong. Therefore, both models yield similar RMSE on the test set despite differing coefficients.














